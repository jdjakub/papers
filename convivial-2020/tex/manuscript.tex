\documentclass[english,submission]{programming}
\usepackage[backend=biber]{biblatex} % Use Biblatex
\addbibresource{example.bib}

\begin{document}

\paperdetails{
  perspective=engineering,
  area={General-purpose programming}
}

\title{What does it take to create with domain-appropriate tools? A case study on the ``OROM'' system.}
\author{Joel Jakubovic}
\affiliation{University of Kent, Canterbury}

\keywords{paper, showcase, lorem ipsum}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002944.10011122.10003459</concept_id>
<concept_desc>General and reference~Computing standards, RFCs and guidelines</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{General and reference~Computing standards, RFCs and guidelines}`

\maketitle

\begin{abstract}
  This paper shows...
\end{abstract}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

As someone who can code, I have already passed the first and most
important hurdle for making full use of the potential of my computer.
However, even in this supposedly empowered state, I am still far away
from feeling the relationship between myself and software as between
artisan and material, free to shape it into any form with effort
proportional to complexity.

One would have thought that software-creation acts like hypothetical
super-intelligent AI. That is: even though we start from a primitive
base in the 50s (or even today), there would surely be a recursive
process of self-improvement, building better software-creation tools
with the existing ones, until an ``expressivity singularity'' where
software becomes a workable material as described.

However, this didn't happen. Or at least, it is happening glacially
slowly. The brute fact is that whenever you want to create software, you
go to a text editor and figure out how to translate your design into
that. The text editors, being software, were written with the help of
previous text editors, and so on. It's undeniable that text editors have
improved, even if you think it peaked with Emacs. We just don't seem
able to go beyond them.

\begin{quote}
You discuss what's wrong with text editor in the next few paragraphs,
but I think it would be useful to give some brief summary here so that
readers don't think that ``we need to go beyond text editors'' is
unmotivated - I think the summary of the text below is that you want to
create software using tools that are closer to the domain of the
software (text is far from everything; visual editors would be closer to
a more visual domain) This general formulation might also nicely frame
some of the things you say next.
\end{quote}

Amdahl's Law generalises the following idea: even when you spend hours
of effort doubling the performance of a component used 1\% of the time,
your reward is a system overall improved by a mere 0.5\%.

Text coding is certainly ubiquitous, the 99\% case in programming, so
you might wonder where I'm going with this. Well, a small improvement to
text editing, if adopted by everyone, certainly does have a massive
\emph{intermediate} effect -- but this only \emph{matters} to the extent
that text was helping the programmers in the first place. If my goal is
to draw or animate pictures, or create a digital synth from a frequency
spectrogram, then giving me the ability to auto-indent my SVG markup is
rather underwhelming as a productivity increase, as it doesn't target
the core of the enterprise that makes it so hard.

My experience of coding, most of the projects requiring shapes (such as
GUIs), leads me to conclude that no matter how much I improve my skill
at a particular language, knowledge of libraries or even general coding
ability, my predicament stays the same. Our basic method of creating
software is optimised for an ever-diminishing proportion of the software
we actually want to make; ill-optimised for the graphics, layout,
interactivity and and basic physics -- more on this later -- that we
usually require.

Whenever I work on these I feel stuck in a box I know I can never escape
from: that box is the text editor, a fixed conduit through which all
\emph{fundamental} changes to my program must pass. It's not a part of
the system I am building, so I can't even make use of features of the
thing I'm developing, to make its own development easier.

Surely the trick is to \emph{use} coding to build something \emph{better
than it}. And then use that, to build something even better. But there
is an enormous breadth and depth of philosophies here, along with many
failed historical attempts to do better -- or at least, ones that failed
to catch on. But even worse than this is that in my very \emph{language}
here I am making the same mistake as the text editor -- speaking in
unqualified terms of ``better'' and ``worse'' as if there really is a
one-size-fits-all solution to software creation!

Of course what we \emph{really} want is the ability for people to create
\emph{in the way that they think is best} in their particular context --
to equip them to feasibly create the tools that suit them for the thing
they want to make. And second-order tools that suit them for making the
first-order tools, etc. It would do no good to replace text-imperialism
with anything-else-imperialism, which is one common interpretation of
calls for alternatives. If someone wants to type out pictures in ASCII,
let them -- whether they do it for a challenge, or even if they find
that more natural for themselves. But equally, if I want to do it
another way, then please give me that affordance.

\begin{quote}
Is programming like `craft'? I guess some artisans make their own tools,
but most likely not in the higher-order sense you are discussing here.
This suggests that programming is different in some sense - but in what
sense?
\end{quote}

This is how I segue into the software artefact for which I have been
attempting to build a natural representation. True, it is a programmer's
artefact, but it is still representative of what any normal person has
to do, insofar as:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Wanting to create a piece of software (for whatever reason)
\item
  Having in mind a natural way to represent it as it's being built.
\end{enumerate}

\begin{quote}
I think it would be good to say (fairly generally) what kind of system
do you imagine we should be using (and goals that your OROM/SVG is
getting closer to). This also clarifies why nothing like that exists
already. I suppose you want something that lets you program in a way
that is ``close to the problem domain'' (i.e.~visually if the domain is
visual) and something that has no arbitrary boundaries
(i.e.~meta-circular?). Boxer is the former, but not the latter;
Smalltalk/LISP is the latter but not the former.
\end{quote}

\hypertarget{the-orom-system}{%
\section{The OROM system}\label{the-orom-system}}

When I first read the paper Open, Reusable Object Models, I was hooked
on its idea of a small but expressive starting system that could be
self-improved into anything. It describes a late-bound, Smalltalk-style
objects and messaging environment that the authors call ``Id'' -- but
which I refer to as OROM, for pronunciation and Googleability.

\begin{quote}
How much does the reader need to know about OROM for the next
paragraphs? I think they need to know there are objects with references
that send messages to each other - this will be clear to those who know
what `Smalltalk-style' means, but not others. It would be useful to give
a very brief summary before getting to the next part of the story.
\end{quote}

The paper as a publication follows the usual patterns. It consists of
mostly prose, several code listings, some diagrams, and full C sources
for a sample implementation at the end. Insofar as this is normal, I
cannot criticise it. But reading this paper, and trying to grasp their
ideas, brought to my attention how inadequate this norm can sometimes
be.

\begin{quote}
The above feels a bit more chatty than necessary - you could probably
shorten the paragraph into just one sentence without losing much.
\end{quote}

In order to understand it in the first place, I repeatedly found myself
drawing extra diagrams. For example, the first acts of the running
system boil down to initialising the three or so objects. This consists
of allocating memory, interpreting it as a C struct and then filling in
fields in a mundane manner. I had great difficulty grasping the
specifics in my head, but when I drew tables in the style of the paper's
diagrams I readily saw what what going on.

Still, areas like messaging semantics could be quite confusing. After
all, should we expect to be able to predict the entire future evolution
of a dynamical system from a static description of its initial state,
such as source code? Is this not why debuggers exist? Having
``de-compiled'' English text and C source code into object diagrams, it
is a shame to have to compile it all back to struct member assignments.

Worst of all, the reference system does not even have text I/O when it
is run, let alone some sort of GUI. That is, the (un)intended user
interface for this project is a C debugger! Faced with the necessity of
adding \emph{some} UI, it seemed a waste of effort to end up with a
system that must be continually polled for its current state at a
terminal prompt. If I naturally think of this system as 2D tables, why
can't that be how the running system looks? I do not have to keep
polling my eyes for what state my diagrams are in.

But further than that -- why can't the system be \emph{built} out of
tables in the first place? Shouldn't this be the main takeaway from the
amount of time we spend prototyping, explaining and designing software
as diagrams on paper? Why must the ``natural representation'' be
restricted to the finished product?

Thus was my natural representation decided. My first attempt to make it
a reality was a partial success: a webpage made of HTML tables, evolved
via JavaScript.

\hypertarget{orom-as-html-tables}{%
\subsubsection{OROM as HTML tables}\label{orom-as-html-tables}}

(OROM/HTML screenshot)

(Describe basic encoding of obj-dicts as tables, use of css class, etc)

Unfortunately, the choice of ordinary HTML as a substrate proved rather
two-edged. I was grateful for the browser's management of graphical
layout, resizable text fields, and keeping the DOM tree synchronised
with what you actually see. This last property enabled me to make the
decision to \emph{directly} encode much of the system state in the DOM,
achieving basic liveness (the thing on the screen is the actual thing)
for the keys and values of obj-dicts.

On the other hand, the browser requires many features to do its job of
rendering complex web pages. And sadly, as its client, I could only make
use of those capabilities which the W3C had decided, at the time of
authorship, were worth the effort exposing in JavaScript. For anything
else, the browser is a black box, and this was very frustrating in the
following case.

\hypertarget{the-radical-concept-of-arrows-that-stay-on-the-shapes}{%
\paragraph{The Radical Concept of Arrows That Stay On The
Shapes}\label{the-radical-concept-of-arrows-that-stay-on-the-shapes}}

A key aspect of the OROM system is that there is an object \emph{graph}.
That is, obj-dicts can have entries pointing to other obj-dicts, without
restriction to a tree structure. Drawing arrows between things to denote
this is a no-brainer, and I wanted it in my substrate for OROM to match
the diagrams.

A small problem to surmount first: even though I could hijack the
\texttt{\textless{}table\textgreater{}} for its display properties, what
element could I hijack to make arrows between arbitrary points? Luckily,
there was SVG at my disposal, which could be persuaded to display over
the HTML. However, another key feature of my intended substrate was to
be able to rearrange and resize the boxes as desired. So I would also
need to detect changes to the position and size of an element.

Bizarrely, there is no such facility provided for HTML elements. This,
despite the fact the browser \emph{needs} this functionality hence it
must reside \emph{somewhere} inside the black box.

Reluctantly, I stuck with my plan B: each object has a numerical ID and
pointers are just fields containing a number, followed using a deref()
function.

At this point it was starting to look like a mistake not to have done
the whole thing in SVG. I would gain full graphical freedom, though also
lose some benefits of the browser's managing it on my behalf. I already
knew from experience the surprising complexity of a DIY approach to
layout, model-view updates, and interactivity. But such an exercise
would be an opportunity to carefully observe this tedium, and
crystallise some of my intuitions about why it is so consistently
frustrating.

\hypertarget{orom-as-files-and-directories}{%
\subsubsection{OROM as files and
directories}\label{orom-as-files-and-directories}}

\begin{quote}
I would probably put this after the SVG part or perhaps even later once
you discuss more general principles (like the idea of polyfilling to
make a domain substrate into a programmable system). Then you can give
this as another example.
\end{quote}

\hypertarget{orom-as-svg-trees}{%
\subsubsection{OROM as SVG trees}\label{orom-as-svg-trees}}

(Screenshot of OROM/SVG showing boxes, code and arrows)

In this version, obj-dicts are encoded as nested SVG
\texttt{\textless{}rect\textgreater{}}s and other elements, reminiscent
of diSessa's Boxer. This was a significant departure from the table
representation, and even though SVG supports (some) nested HTML via
\texttt{\textless{}foreignObject\textgreater{}}, I actually preferred
the possibility of multiple levels of nesting.

OROM/SVG more or less realises my desired substrate for implementing
OROM. Needless to say, this version was far more challenging and took
much longer to reach a satisfactory state. However, it is precisely this
drudgery that brings me to a better understanding of this paper's
question: \emph{what has it taken?} I shall discuss this in the form of
broad patterns or themes that stand out to me from my development
experience, hammered home by these OROM projects.

\begin{quote}
Why is OROM/SVG more challenging? I think it would be useful to sketch
here, because it reveals some useful principles. There is more
polyfilling (arrows and resizing is not in the SVG substrate itself),
but at least it's possible to add those (unlike in HTML)
\end{quote}

\hypertarget{inevitable-requirements-of-most-software-and-the-work-we-must-do-to-meet-them}{%
\subsection{Inevitable Requirements Of Most Software (and the work we
must do to meet
them)}\label{inevitable-requirements-of-most-software-and-the-work-we-must-do-to-meet-them}}

As programmers, we have a maxim along the lines of: when you find
yourself repeating the same thing over and over again, factor it into a
``thing'' and let the computer do the duplication. Some consider the
``design pattern'' to be simply what a ``language feature'' looks like
when it's not part of the syntax.

\begin{quote}
The jump from very general abstraction idea to design patterns and
language features in the above is not exactly clear - it would be good
to help the reader understand why are you talking about this here.
\end{quote}

As I developed OROM/SVG I found myself implementing many such patterns.
It seems that languages are often at the wrong level of abstraction for
the requirements of modern software, necessitating the same boilerplate
per project just to get up to basic functionality. This burden either
falls on the author, or on the wider community to build and maintain
higher-level frameworks, syntax extensions, etc.

By contrast, these languages seem much better adapted to e.g.~batch-mode
printf()-centric applications. That they fail for the \emph{common case}
should be concerning. I will now present what seem to be the inevitable
demands of this common case.

\begin{quote}
I'm not sure I understand what the above opening is trying to say - I
suppose it might be because it is quite high-level/general. What is this
section about more specifically? Is it about what certain kind of
software consists of and what we need to build that? I would also be a
bit careful about things like ``most modern software'' below. It would
be good to instead characterize what kind of software are you interested
in - end-user graphical systems? (a microservice or something like that
does not need SVG\ldots{})
\end{quote}

\hypertarget{as-a-mere-consumer}{%
\subsubsection{As A ``Mere Consumer''}\label{as-a-mere-consumer}}

I begin by describing expectations of software that even ``end-users''
hold, consigned as they are to more or less accept what we give them.

\hypertarget{retained-mode-vector-graphics}{%
\paragraph{Retained-Mode Vector
Graphics}\label{retained-mode-vector-graphics}}

Most software is designed for the subset of people who have a colour
display they can perceive. So right away it is going to require ways to
draw coloured shapes. There are usually libraries for this (though note:
not part of the language), but some only provide \emph{immediate mode}:
commands to instantaneously rasterise pixels to a buffer. This is not
enough for modern software, as we often expect animation, or at least to
see things change as we interact. Most often we wish to see \emph{small
changes} to the \emph{same} shapes, rather than completely different
shapes altogether; the reification that this requires to persist between
frames, is known as \emph{retained mode.}

On this requirement, SVG fits the bill very well. Although it is not
part of the JavaScript language \emph{per se}, it is a standard and
widely supported technology of the Web \emph{platform}. We can observe
that anyone with a browser \emph{in principle} has access to a powerful
vector graphics editor -- just one with no GUI. I will return to this
later.

The SVG tree keeps the nice properties of the DOM, such as updating the
display when shape parameters are changed. This is well-adapted to
``I/O-bound'' software like mine, where things change only in response
to user input. If I wanted animation, this boils down to a regular
``advance simulation'' signal, and would require setting up some
rendering loop. Alternatively, there is the W3C's chosen ontology of CSS
animations, but see section ??.

\hypertarget{basic-assumptions-about-physical-objects}{%
\paragraph{Basic Assumptions About Physical
Objects}\label{basic-assumptions-about-physical-objects}}

In any software making use of vector graphics, there is usually some
level of ``physics'' expected by users. This need not be nearly as
exhaustive as the word ``physics'' might imply, as in e.g.~physics
engines for games; I feel it is important to recognise it for what it is
instead of conceiving physics as an inherently complex thing to be found
only in specialised simulations.

\begin{quote}
People might not find this easy to believe - but I think you can
convince them by saying that pretty much all mobile phone apps have some
sort of ``phone touch physics'' (with sliding, opening menus etc.).
\end{quote}

All humans learn a basic set of expectations about the things they see
around them. Some of these, such as ``things fall down'', are not
generally appropriate to software UIs -- perhaps because the screen has
a role in our lives a more of a table work surface rather than a
vertical wall, even if it is vertical in real life.

The level of physics in software tends to not involve force, or mass, or
very much at all, merely position and space. One thing that all usable
software must do, for example, is not crush many visually complex
shapes, such as lines of text, into the same region, since it becomes
unreadable. Such concepts of ``solid objects do not intersect'' or
``only things at different layers may overlap'' are basic rules
inherited from the real world of graphical presentation.

I feel the need to point this out, because by default the computer does
not know even the most obvious things about how space works, so we must
laboriously algorithmise this intuitive concept. This is not only true
in the case of 2-dimensional visual domains, but even in the
1-dimensional case of memory allocation. The physics of 1D memory are
something like this:

\begin{itemize}
\tightlist
\item
  This number range 0000-FFFF is like a space, numbers = points
\item
  Every point has at most one owner block
\item
  These blocks are contiguous, finite ranges (i.e.~1D boxes)
\end{itemize}

Hence the boilerplate involved to realise this in any domain with
something resembling space. Far from being a niche topic in games and
graphics, spatial partitioning algorithms and data structures have
surprising relevance to more ordinary software. Both memory allocation
and graphical layout are essential to today's; shame that only one of
those has been recognised as such and entered the runtime of modern
languages.

\hypertarget{translationally-rigid-bodies}{%
\subparagraph{Translationally Rigid
Bodies}\label{translationally-rigid-bodies}}

When you have both a screen and a pointing device (e.g.~touch or mouse),
immediately it becomes worth having ways to move things around in at
least a minimally realistic way. We can debate the appropriateness of
Direct Manipulation (DM) for various situations. But it does make a lot
of sense in simple cases, such moving around subdivisions of space
(e.g.~windows) or elements of a graphical design.

In OROM, the obvious candidate for this is the obj-dicts, plus all
nested boxes in OROM/SVG. If I move the top-level rect, then I expect
its children to move with it. This is simply the translational physics
of rigid bodies: this set of points all move together. Of course, proper
rigid bodies might also rotate and have mass, but this is usually
undesirable for UI elements.

Translational rigidity can be expressed as the points X and Y always
having the same displacement from each other. Or, when one point is
moved, the rest also move by the same delta. This is a problem of
preserving the relationship over time, which was a significant area of
OROM/SVG.

\hypertarget{maintaining-relationships-over-time}{%
\paragraph{Maintaining Relationships Over
Time}\label{maintaining-relationships-over-time}}

The model of state-mutation present in most imperative languages is what
I call ``dumb'' state. The language provides an affordance to change any
part of the state to a new value, but nothing else.

What more could there be? Well, in \emph{every} software system there
are certain rules, or ``invariants'' of internal consistency, such as
``translational rigidity'' above. Often, changes to any part of the
system are permissible, but only if connected or dependent parts of the
state change in response.

The job of keeping track of who depends on whom can fall either on the
programmer, or the computer. If the programmer has to do this, they can
only go so far managing and simulating in their head. As systems grow
more complex, it is only natural to try and make the computer more
intelligent to do this work. What I am building up to is that whenever
we (or I) consider constraints, or ``reactive'' programming, or the
``Observer'' pattern as things you only wheel out on special occasions,
we only deceive ourselves into doing the same work less explicitly. It
seems that such ``live state'' should be the expected common case for
software development.

If a platform does not provide a means to causally link and unlink bits
of live state, then this must form part of the standard boilerplate.

Such was the case in OROM/SVG. I implemented the system in an OOP
fashion, and the Observable class is the most widely used. It wraps a
current value and a list of subscribers, notifying them when it changes.
It can be seen that this supplanted JavaScript's own ``dumb'' state by
the frequency of the change() function throughout the code.

Getting an object to follow the mouse pointer, for example when
dragging, is conceptually very simple: an ``always equal'' relation. In
OROM/SVG founded on live-state, this can be expressed in much the same
way:

\begin{verbatim}
subscribe(object.position, pointer.position);
\end{verbatim}

(By default, an Observable A responds to a change from Observable B by
adopting B's new value.)

\hypertarget{nut-cracking-with-sledgehammers}{%
\paragraph{Nut-Cracking With
Sledgehammers}\label{nut-cracking-with-sledgehammers}}

Speaking about vector graphics, physics, layout and constraint
maintenance might give the impression of high \emph{conceptual}
complexity at the heart of even simple software. This is not quite true,
which makes it all the worse that there is yet still immense
\emph{incidental} or implementation complexity.

We are conditioned to only think of these in their most general forms.
The vector graphics I use in OROM/SVG are just rects, lines, circles and
text; a fraction of the full capability of SVG. The ``basic physics'' I
use is dwarfed by fully general 2D or 3D physics engines. The only
layout algorithm I had the patience to implement was a simple way to
expand a list of boxes to fit in a new child at the bottom; the
affordance to place and size boxes \emph{manually} staves off the rest
for the time being! Yet search for material on layout algorithms and it
can seem like Fully General Linear Inequality Solvers like Cassowary are
all there is.

The inevitable requirements I suggest here, do \emph{not} necessitate
Fully General anything. In fact, such representations would make it more
cumbersome to express what I wanted in OROM. As the old wisdom goes,
there's no point expressing a simple regex search as an arbitrary Turing
machine; my boxes don't \emph{have} a moment of inertia or a mass and I
don't \emph{need} a linear optimisation solver for my space management
-- for the time being.

Unfortunately, there doesn't seem to be much interest on the
smaller-scale instances of these problems. I suggest the name
``geometric physics'' for the subset of physics I have mentioned.

\begin{quote}
I think the above section talks about 3 things - (i) it describes some
of the things that you had to implement for OROM/SVG, (ii) it talks
about some general patterns that arise there (like shapes and physics)
and (iii) it mentions some implementation details. I found it a bit
confusing to have these all mixed together. It reads a bit as if you
were jumping from one random topic that you encountered when
implementing this - but that makes the section very specific to your
implementation. How can we make it clear that there are more general
principles here? I think it might be better if you started with some
larger example (hypothetical is fine!) to illustrate what kind of things
you'd like to build - e.g. imagine you're building PowerPoint in OROM -
what would be some of the things you'd need? (for both the end
application and the system that you're using to build it). This would
then nicely motivate the more specific topics (graphics, physics,
relationships).
\end{quote}

\hypertarget{as-a-developer}{%
\subsubsection{As A ``Developer''}\label{as-a-developer}}

I now turn to some things that were perhaps inevitable, in any case
useful, as the \emph{implementor} of OROM/SVG.

\hypertarget{practicality-vs.-high-aesthetics}{%
\paragraph{Practicality vs.~High
Aesthetics}\label{practicality-vs.-high-aesthetics}}

There is the view that insists on provably-correct code written with
strange Unicode symbols, and at the opposite end there is the attitude
of ``get the job done'', high style be damned. Each has its merits, but
for my task I quickly saw the latter route to be most suitable.

My favourite example of this has been the ``encoding'' of obj-dicts as
DOM trees containing particular arrangements of children. In OROM/HTML,
I would gleefully get/set state by calling code like this:

(Listing involving CSS, element querying / munging)

and in OROM/SVG the story is similar. It has the advantage of the
powerful developer tools included in browsers, and avoids the need to
manually synchronise separate Model and View for such encoded state.
With a little work, it also allowed the system a large degree of
externalisability -- see Section ??.

Contrast this with an approach that had to re-synthesise bits of the DOM
whenever something changed. Admittedly this does still happen to a
certain extent, but at least it was minimised where obvious.

Another example of practicality was in the issue of positioning and
sizing. In Section ?? I mentioned that I was able to sidestep some
layout work by offloading it to the user. While building the system I
tended to want to draw boxes into existence, and for that my brain's
existing aesthetic algorithms can decide where they go. There are areas
where the machine must be able to figure this out itself -- I don't want
to be constantly bothered by prompts to place boxes as they are
allocated by some running code -- but for one-offs it is a very
convenient way to do a little less yak-shearing.

\begin{quote}
I think you could adapt the above discussion a bit and focus less on
what you don't want (high modernism) and more on what you want - what is
that? I think it's simplifying the problem along a different axis - to a
certain domain with relatively simple interactions and aesthetics. (So
rather than trying to make minimal formal calculus that's provably
correct, you're trying to make minimal working interaction model that's
interesting)
\end{quote}

\hypertarget{positioning-and-sizing}{%
\paragraph{Positioning and Sizing}\label{positioning-and-sizing}}

The simple desire to move and resize boxes with the mouse motivated a
lot of the live-state and geometric physics ideas. This problem could be
considered a microcosm of OROM/SVG: what's a natural way I
\emph{conceive} of this behaviour, and could I implement it that way?

\begin{quote}
The above point (moving motivated live state and physics ideas) would be
a lot more obvious if you followed the structure with motivating example
first followed by an analysis\ldots{}
\end{quote}

It starts with a consideration of translational rigidity. In its most
primitive form, this is a relation between two points. Thus it is
natural to draw a line or ``rod'' between them. It seems that this rod
transmits changes in one of its endpoints directly to the other
endpoint. Since they both feel the same deltas, the displacement vector
between them is preserved.

(Diagram of delta transmission)

I like to see what I'm doing, so I wanted these rods to be visible and
thus somehow \emph{present in the SVG}. This was not hard; in SVG I draw
a \texttt{\textless{}line\textgreater{}}, though the background
machinery of the Rod class does all the work. There is also a Point
class that is used wherever manipulable points (SVG circles) are
intended.

Resizing (of boxes at least) could be achieved through rods that stay
horizontal or vertical. In the language of ``small differences'' spoken
by my live-state infrastructure, this is expressed as a rod
``transmitting deltas'' in the vertical and horizontal, ``absorbing''
the other component into itself. Mirroring a DOM rect to these rods is
as simple as subscribing its width and height to horizontal and vertical
rods' length Observables. This way, boxes can be resized from whatever
corner is convenient.

(Box demonstrating all three rod types)

I colour such ``half-rigid'' rods yellow and fully rigid rods red. The
``absorb all endpoint changes'' case is coloured green.

\begin{quote}
I don't think you explained what `half-rigid' means yet.
\end{quote}

Unfortunately, with these rods came possibly the most frustrating
technical challenges of the entire system. Initially I hoped to move
boxes as rigid bodies by temporarily making their border rods rigid.
However, the four border rods form a cyclic graph, as rods are not
directed. This, coupled with the unintended depth-first semantics of
Observable notification (a result of JavaScript function calls in a
loop), led to duplicate deltas applied twice and other nightmares. This
is the tip of an entire research iceberg stretching from Functional
Reactive Programming to internet routing and distributed algorithms.

I did manage to surmount this through kludging and compromise. But I do
not know whether these problems are merely a consequence of some design
decision I could change to escape it, or if they are intrinsic to my
(modest) UI goal.

\begin{quote}
This section reads more as an implementation detail - even though it
makes some interesting points about the physics \& rods system. However,
it might better to have it in a section that's more about implementation
than about general principles. Or perhaps you could restructure Section
3 to have (i) example (ii) general lessons (like physics etc.) and (iii)
specific points about implementation (e.g.~your concrete physics system
with rods, what you had to `polyfill')
\end{quote}

\hypertarget{visible-coordinate-systems}{%
\paragraph{Visible Coordinate
Systems}\label{visible-coordinate-systems}}

Rigidity in a flat world of sibling shapes is somewhat straightforward.
However, rigidity in the SVG world is more involved.

First of all, SVG shapes (e.g.~\texttt{\textless{}rect\textgreater{}})
are strictly leaf nodes, so if I wish to nest boxes within boxes, the
visible ``box'' must be a mere \emph{accessory} to the nestable element,
in my case a \texttt{\textless{}g\textgreater{}} (group). This means
that instead of resizing the x, y, width, height attributes of the
\texttt{\textless{}rect\textgreater{}}, only its width and height change
along with the transform attribute of its \emph{parent}
\texttt{\textless{}g\textgreater{}}. This was not too bad; just
subscribe this attribute, instead of the
\texttt{\textless{}rect\textgreater{}} position, to the top-left Point
handle.

All child elements of a node transform with it, so already SVG has baked
in a basic facility for translational rigidity. This is only available
as a tree hierarchy, but it is still useful. However, it conflicts with
my early decision to have Point objects all share the global co-ordinate
system. This way, connecting ``this point'' to the mouse pointer, etc.
is not infuriating to express. Still, it was necessary in the case of
certain elements -- especially those which must transcend the tree
structure altogether, like arrows between boxes -- to bite this bullet,
one way or another.

\begin{quote}
Maybe mention that this is caused by the mismatch between the substrate
and the programming system (substrate is tree, but programming is
graphs)?
\end{quote}

Again, I return to how we tend to work things out in the freedom of
paper. Co-ordinate systems, here merely positionally displaced, have
their origins here and there and have vectors between them. The rods
thus far let me visually express relations between global Points; now
was a question of expressing one global Point as a displacement from
another (the \texttt{\textless{}g\textgreater{}} transform). New rod
Observables p2\_from\_p1 and p1\_from\_p2 do the vector subtraction,
which can then be propagated as local co-ordinates to children. It is
nicer to express the relation (as well as see it!) this way -- see
Figure ?? above.

\hypertarget{context-appropriate-ontologies}{%
\paragraph{Context-appropriate
ontologies}\label{context-appropriate-ontologies}}

\ldots{}

Talk about: fixed ontologies (a rect IS x,y,w,h, and nothing else!)
imposed on programmer

Example from 3.1.1. mention: CSS animations?

``One size fits all'' rears its ugly head again.

I want: top-left, top-right, bot-left, bot-right \ldots{}

Instead of explaining ``the x y is the top left blah blah'' in English
in the W3C spec, they should make these relationships \emph{machine
readable} with information like:

A Rect is

\begin{itemize}
\tightlist
\item
  a Polygon
\item
  defined by 4 degrees of freedom x y width height
\item
  where the four vertices are

  \begin{itemize}
  \tightlist
  \item
    \texttt{{[}x,y{]}} ``top-left''
  \item
    \texttt{{[}x+width,y{]}} ``top-right''
  \item
    etc
  \end{itemize}
\end{itemize}

Reference to paper related to Antranig involving knitting\ldots{}?

\ldots{}

\hypertarget{extensional-functions}{%
\paragraph{Extensional Functions}\label{extensional-functions}}

aka dicts, Maps, std::vector, \ldots{}

\ldots{} \ldots{} Some parts of the OROM authors' C code were confusing
until I realised they were just the guts of a basic associative-array
implementation. \ldots{} \ldots{}

Inevitability / ubiquity of state descriptions that are at trees of
strings with pointers

Antranig ``natural co-ordinates''?

OROM/SVG as Boxer-like extensional functions

\hypertarget{externalisability}{%
\paragraph{Externalisability}\label{externalisability}}

Reference Antranig's stuff on this. How I dealt with event listeners etc

Necessity to development: frustrating to find bug, tear down, refresh,
lose work, build it up again!

Inspect Element: copy to save works, arbitrary edits \emph{should} work
(cuz it's on demand)

Work required to parse SVG trees into objects ``on demand''

Started heavyweight, ended lightweight (generate rods on demand, let SVG
move children)

\hypertarget{polyfilling}{%
\paragraph{Polyfilling}\label{polyfilling}}

Very advantageous to permit this e.g.~JavaScript

Stupid lack of XYZ in JS or SVG due to come out in the next spec, etc

Syntax sugar (SVG tree ops, vector ops)

\hypertarget{the-orom-system-as-a-part-of-the-solution}{%
\subsection{The OROM system as a part of the
solution}\label{the-orom-system-as-a-part-of-the-solution}}

Meta-circularity as empowerment, not just ``cool''

The allure

Other themes e.g.~homoiconicity of code and data

Problems: cognitive vs.~``official'' complexity

Maxwell's eqns metaphor: omits the years of study compressed into the
math symbols!

And yet, the allure of the Maxwell's Eqns of Software?

``Taming'' SVG - ability to use SVG in a vaguely sane way (vector
graphics editor)

Similarly for e.g.~3D OpenGL, Web Audio, sockets \ldots{}

\hypertarget{tensions-between-philosophies}{%
\subsubsection{Tensions between
philosophies}\label{tensions-between-philosophies}}

\hypertarget{stable-or-emergent-antranig-against-oop-dispatch}{%
\paragraph{Stable or emergent? (Antranig against OOP
dispatch)}\label{stable-or-emergent-antranig-against-oop-dispatch}}

\hypertarget{turing-complete-antranig-against-this}{%
\paragraph{Turing-complete? (Antranig against
this)}\label{turing-complete-antranig-against-this}}

\hypertarget{messaging-vs-read-write}{%
\paragraph{Messaging vs Read-Write?}\label{messaging-vs-read-write}}

\hypertarget{misc}{%
\section{Misc}\label{misc}}

A note about DIY and libraries

My other problem was one of cost-benefit. My aim was to \ldots{}


\acks
I want to thank ....

\printbibliography
\end{document}
